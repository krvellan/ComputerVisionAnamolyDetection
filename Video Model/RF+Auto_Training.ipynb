{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import figure\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import glob\n",
        "from keras.preprocessing import image as kimage\n",
        "from roboflow import Roboflow\n",
        "import json\n",
        "from time import sleep\n",
        "from PIL import Image, ImageDraw\n",
        "import io\n",
        "import base64\n",
        "import random\n",
        "import requests\n",
        "from os.path import exists\n",
        "import os, sys, re, glob\n",
        "from IPython.display import clear_output\n",
        "import os, urllib.request"
      ],
      "metadata": {
        "id": "igI2HhfIYT7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT6dn6Gl7WpI"
      },
      "source": [
        "# FFMPEG Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFF8tkyK70_D"
      },
      "source": [
        "### FFMPEG Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "0c-2lgoHDiRp",
        "outputId": "bd55c5f1-f8f9-449f-94aa-3f2fc655e8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation finished.\n"
          ]
        }
      ],
      "source": [
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Installing Dependencies...\", ty='twg')\n",
        "os.system('pip install git+git://github.com/AWConant/jikanpy.git')\n",
        "os.system('add-apt-repository -y ppa:jonathonf/ffmpeg-4')\n",
        "os.system('apt-get update')\n",
        "os.system('apt install mediainfo')\n",
        "os.system('apt-get install ffmpeg')\n",
        "clear_output()\n",
        "print('Installation finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Access video"
      ],
      "metadata": {
        "id": "iReTCPV9ET_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!mkdir videos_to_infer\n",
        "!mkdir inferred_videos\n",
        "!mkdir videos_to_infer_two\n",
        "!mkdir inferred_videos_two\n",
        "\n",
        "%cd videos_to_infer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8ASYbq3oBoU",
        "outputId": "090ee533-a6ba-4cbe-a25d-66a3b5956473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/videos_to_infer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Link your Google Drive to upload files to/from Google Drive\n",
        "\n",
        "\n",
        "*   process outlined in the next 2 cells\n",
        "\n"
      ],
      "metadata": {
        "id": "I_pqrClDlEHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL - link your g-drive to pull videos from\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "e15g_5kXiChj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0a1044-d73d-4e3c-c0ce-2d56f2ae7648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL - copy your videos from g-drive to /content/\n",
        "!cp \"/content/gdrive/MyDrive/Data 298B/main/training.mp4\" \"/content/videos_to_infer\""
      ],
      "metadata": {
        "id": "NvBwDrfjiKgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/gdrive/MyDrive/Data 298B/main/test.mp4\" \"/content/videos_to_infer_two\""
      ],
      "metadata": {
        "id": "OqWq7aG2EI4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Break down video frames into images"
      ],
      "metadata": {
        "id": "bjhxbtGXlW4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SndB9usRtjW4"
      },
      "outputs": [],
      "source": [
        "os.environ['inputFile'] = \"/content/videos_to_infer/training.mp4\"\n",
        "\n",
        "!ffmpeg  -hide_banner -loglevel error -i \"$inputFile\" -vf fps=30 \"$inputFile_out%04d.png\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \n",
        "%cd /content/videos_to_infer_two"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrxQmVuS9AXZ",
        "outputId": "2952b6af-955f-4809-922d-8f645f68750c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/content/videos_to_infer_two\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['inputFile_two'] = \"/content/videos_to_infer_two/test.mp4\"\n",
        "\n",
        "!ffmpeg  -hide_banner -loglevel error -i \"$inputFile_two\" -vf fps=30 \"$inputFile_out%04d.png\""
      ],
      "metadata": {
        "id": "t9gHWwlcEZrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main - Obj Detection"
      ],
      "metadata": {
        "id": "3WNOtO4DYXI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# workspace code\n",
        "from roboflow import Roboflow\n",
        "import json\n",
        "\n",
        "rf = Roboflow(api_key=\"7CZtbBaqqkv8yKNnDTR9\")\n",
        "project = rf.workspace().project(\"anomaly-detection-2.0\")\n",
        "\n",
        "# grab the model from that project's version\n",
        "model = project.version(1).model\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr4oEIKpYX2q",
        "outputId": "21ee2672-5f91-49cf-893b-ad2ecafa5fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "{\n",
            "  \"id\": \"anomaly-detection-2.0/1\",\n",
            "  \"name\": \"Anomaly detection 2.0\",\n",
            "  \"version\": \"1\",\n",
            "  \"classes\": null,\n",
            "  \"overlap\": 30,\n",
            "  \"confidence\": 40,\n",
            "  \"stroke\": 1,\n",
            "  \"labels\": false,\n",
            "  \"format\": \"json\",\n",
            "  \"base_url\": \"https://detect.roboflow.com/\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# HELPER FUNCTIONS BLOCK\n",
        "def draw_boxes(box, x0, y0, img, class_name):\n",
        "    # OPTIONAL - color map, change the key-values for each color to make the\n",
        "    # class output labels specific to your dataset\n",
        "    color_map = {\n",
        "        \"floor\":\"red\",\n",
        "        \"door\":\"blue\",\n",
        "        \"wall\":\"yellow\",\n",
        "        \"person\":\"green\"\n",
        "    }\n",
        "\n",
        "    # get position coordinates\n",
        "    bbox = ImageDraw.Draw(img) \n",
        "\n",
        "    bbox.rectangle(box, outline =color_map[class_name], width=5)\n",
        "    font = ImageFont.truetype(\"/content/gdrive/MyDrive/Data 298B/new/arial.ttf\", 20) \n",
        "    bbox.text((x0, y0), class_name, fill='white', anchor=None , font = font, spacing = 20)\n",
        "\n",
        "    return img\n",
        "\n",
        "def save_with_bbox_renders(img):\n",
        "    file_name = os.path.basename(img.filename)\n",
        "    img.save('/content/inferred_videos/' + file_name)"
      ],
      "metadata": {
        "id": "eg8_hVHsYadd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_text_to_image(image_path, text, position):\n",
        "    # Open an Image\n",
        "    img = image_path\n",
        "\n",
        "    # Initialize ImageDraw\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Specify Font \n",
        "    # This example uses the built-in \"Arial\" font\n",
        "    # Make sure to have this font file (arial.ttf) in your working directory or give the absolute path\n",
        "    font = ImageFont.truetype(\"/content/gdrive/MyDrive/Data 298B/new/arial.ttf\", 20)\n",
        "\n",
        "    # Add Text\n",
        "    draw.text((10, position), text, fill=\"#25A1D4\", font=font)\n",
        "\n",
        "    # Save the Image\n",
        "    return img"
      ],
      "metadata": {
        "id": "h43uW_TIzPII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "2CkY3Z7UzWnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set path and get all image files\n",
        "file_path = \"/content/videos_to_infer/\"\n",
        "extention = \".png\"\n",
        "globbed_files = sorted(glob.glob(file_path + '*' + extention))\n",
        "\n",
        "# Prepare for training data\n",
        "class_data = {\"wall\": [], \"floor\": [], \"door\": []}\n",
        "\n",
        "# Iterate over all images\n",
        "for img_path in globbed_files:\n",
        "    predictions = model.predict(img_path, confidence=35, overlap=0).json()['predictions']\n",
        "    newly_rendered_image = Image.open(img_path)\n",
        "\n",
        "    for prediction in predictions:\n",
        "        x0 = prediction['x'] - prediction['width'] / 2\n",
        "        x1 = prediction['x'] + prediction['width'] / 2\n",
        "        y0 = prediction['y'] - prediction['height'] / 2\n",
        "        y1 = prediction['y'] + prediction['height'] / 2\n",
        "        box = (x0, y0, x1, y1)\n",
        "\n",
        "        # Crop each ROI and resize to 64x64 (or any size that your model accepts)\n",
        "        roi = newly_rendered_image.crop(box).resize((64, 64))\n",
        "\n",
        "        # Convert ROI to numpy array and normalize to [0,1]\n",
        "        roi_np = np.array(roi) / 255.0\n",
        "        class_data[prediction['class']].append(roi_np)\n",
        "\n",
        "        # Store the ROI in the appropriate list depending on its class\n",
        "        #if prediction['class'] == 'door':\n",
        "            #train_data_door.append(roi_np)\n",
        "        #elif prediction['class'] == 'wall':\n",
        "            #train_data_wall.append(roi_np)\n",
        "        #elif prediction['class'] == 'floor':\n",
        "            #train_data_floor.append(roi_np)\n",
        "\n",
        "        # Draw bounding boxes and add text to the image\n",
        "        newly_rendered_image = draw_boxes(box, x0, y0, newly_rendered_image, prediction['class'])\n",
        "        newly_rendered_image = add_text_to_image(newly_rendered_image, 'Status: Normal')"
      ],
      "metadata": {
        "id": "vtPKEEAbhE8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_autoencoder():\n",
        "    input_img = Input(shape=(64, 64, 3))\n",
        "    l1_lambda = 0.001  # Define the lambda for L1 regularization\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(l1_lambda))(input_img)\n",
        "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(l1_lambda))(x)\n",
        "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = Conv2D(16, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(l1_lambda))(encoded)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(l1_lambda))(x)\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', kernel_regularizer=regularizers.l1(l1_lambda))(x)\n",
        "\n",
        "    # Autoencoder = Encoder + Decoder\n",
        "    autoencoder = Model(input_img, decoded)\n",
        "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "metadata": {
        "id": "Z2sL1OHxDlOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set path and get all image files\n",
        "file_path = \"/content/videos_to_infer_two/\"\n",
        "extention = \".png\"\n",
        "globbed_files = sorted(glob.glob(file_path + '*' + extention))\n",
        "train_df = pd.read_csv('/content/gdrive/MyDrive/Data 298B/training_data.csv')"
      ],
      "metadata": {
        "id": "V6r7qcuCDlQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert list of numpy arrays to single numpy array\n",
        "for class_name in class_data.keys():\n",
        "    class_data[class_name] = np.array(class_data[class_name])\n",
        "\n",
        "# Create and train an autoencoder for each class\n",
        "autoencoders = {}\n",
        "history_dict = {}"
      ],
      "metadata": {
        "id": "6xmJCOjQe9qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: convert categorical features if necessary\n",
        "le = LabelEncoder()\n",
        "\n",
        "train_df['location'] = le.fit_transform(df['location'])\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "features = train_df.drop('target_label', axis=1)\n",
        "target = train_df['target_label']\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# Creating and training the Random Forest model\n",
        "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42)\n",
        "classifier.fit(features_train, target_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "target_pred = classifier.predict(features_test)\n",
        "\n",
        "# Model accuracy\n",
        "print('Model Accuracy is', accuracy_score(target_test, target_pred)*100)\n",
        "\n",
        "# Saving the model\n",
        "joblib.dump(classifier, 'random_forest_model.pkl')"
      ],
      "metadata": {
        "id": "usSsx69xfXkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name, data in class_data.items():\n",
        "    print(f\"Training autoencoder for {class_name}...\")\n",
        "\n",
        "    # Create an autoencoder for this class\n",
        "    autoencoder = create_autoencoder()\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    train_data = data[:int(len(data)*0.8)]\n",
        "    val_data = data[int(len(data)*0.8):]\n",
        "\n",
        "    # Train the autoencoder\n",
        "    history = autoencoder.fit(train_data, train_data, epochs=500, batch_size=32, validation_data=(val_data, val_data), callbacks=[early_stopping_callback])\n",
        "\n",
        "    # Store the trained autoencoder\n",
        "    autoencoders[class_name] = autoencoder\n",
        "\n",
        "    # Store the training history\n",
        "    history_dict[class_name] = history\n",
        "\n",
        "# Now we have a separate autoencoder for each class and we can use them to infer new images\n",
        "\n",
        "# Save the models\n",
        "for class_name, autoencoder in autoencoders.items():\n",
        "    autoencoder.save(f\"{class_name}_autoencoder.h5\")"
      ],
      "metadata": {
        "id": "-t7cINyFe_LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, title):\n",
        "    loss_values = history.history['loss']\n",
        "    val_loss_values = history.history['val_loss']\n",
        "    epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(epochs, loss_values, 'r', label='Training loss')\n",
        "    plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss for ' + title + ' object')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qCxU6AYVe_rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the history for 'door' class\n",
        "plot_history(history_dict['door'], \"door\")\n",
        "plot_history(history_dict['wall'], \"wall\")\n",
        "plot_history(history_dict['floor'], \"floor\")"
      ],
      "metadata": {
        "id": "goofZ39nfB1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lh4LySBHDlf9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}